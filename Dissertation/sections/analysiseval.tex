\chapter{Analysis and Evaluation}

Despite the early positive results from the RMSE values of the linear regression model once the classes did not appear to be separable in the visualisations produced it seemed unlikely that the classification attempt would be highly successful. Although previous work \cite{Gomez2013} had related the relative powers to the age of the subject via a quadratic relationship, an attempt to fit a quadratic model to the data we had was less successful than the linear model. It is worth noting that the previous study investigated the relative powers as a function of age, looking at each relative power band separately and thus it simple linear regression rather than multiple regression, and our study investigated the age as a function of the relative powers. The difference are because of the different aims of the studies, they were investigating the effect of age on the relative powers, whereas we were attempting to predict the age from the relative powers.

Although the RMSE values obtained from the linear model do agree with our expectations (the healthy data has significantly less error than the MCI data which has less error than the AD data), they are still quite large especially the value of 14.86 years for the healthy dataset, which given that the model is trained on the healthy data should be very low as the model should be able to predict it very well. Of course given that the diseased patients had significantly worse RMSE values that should not prevent the residual values from being used to classify the disease, but it does show that the data is perhaps quite noisy or the relative powers just have little information about the age of the subject.

The correlation observed between the residuals of the model and the cognitive test scores for the diseased patients was very weak as demonstrated by the poor $r^2$ values, therefore it appears that the difference in relative powers between diseased and healthy patients (which the model residual effectively acts as a proxy for) is not sufficient to determine the severity of the disease. Also it should be noted that the predicted age for the diseased patients was often lower than their actual age. This doesn't fit the hypothesis that the difference in the model would reflect accelerated brain ageing.

It is interesting to see that the classifier trained solely on the residuals from the model did better than that using the whole feature set. This is probably due to the \textit{curse of dimensionality} in which the high-dimensional space of the larger feature set causes the classifier to perform poorly \cite{Witten2011}. The residuals of the linear regression models are based on the relative powers and so in principle should contain much the same information (i.e. it is essentially a form of feature engineering), but the dimensionality has been reduced, thus allowing the classifier to perform better. It should also be noted that boosted algorithms perform very poorly in the case of label noise (mislabelled training examples) \cite{Long2009}. This is important to note in our case as Alzheimer's and MCI diagnosis is not an exact science and some of the subjects may have been misdiagnosed.

A `dumb' classifier that simply predicted all the patients to be healthy would outperform it (based purely on classification accuracy). The fact that the classifier didn't just learn to classify all patients as healthy shows that RUSBoost is successful at dealing with skewed classes but nonetheless the classifier is still not successful enough to be useful, especially in medical diagnosis which requires a classifier to be both highly sensitive and highly specific (i.e. a low false positive and false negative rate) because of the economic costs of unnecessary tests and treatments alongside the psychological strain of a false diagnosis whilst a false negative result can have potentially fatal consequences.

The $F_{1M}$ score that takes false positives and false negatives into account and is thus often a better figure of merit than classification accuracy (especially for medical diagnosis where the mistakes are important as previously discussed) favoured our classifiers over the dummy classifier and random guessing as mentioned previously they are far from the quality required for medical diagnosis.

There were some difficulties with the study, for example because MCI and Alzheimer's are typically diagnosed in old age this means that some of the inaccuracy of the model may simply be due to the fact that it was also trained on younger healthy ages, whereas the AD and MCI patients were all relatively old. This is compounded by the fact that most of the control subjects were younger perhaps because students are common participants in such experiments and they tend to be young.

Furthermore given the nature of resting-state MEG experiments it is somewhat common for the subjects to fall asleep as it is dark, the subject is asked to remain immobile and the experiment may last quite some time. This is not a problem in itself if it is correctly reported, but even with a camera it can be very difficult to notice if the subject has fallen asleep and while healthy subjects are usually aware that they have fallen asleep upon waking, diseased patients who are cognitively impaired may fail to report it thus leading to contaminated data. This often shows up in the power spectra as an increased power in the lower frequency bands, but manually inspecting every power spectrum is not feasible given the vast number of subjects, channels and epochs. Given that the instances in which the subject falls asleep are rare compared to the awake state it is possible that in the future one could attempt to use anomaly detection methods\cite{Chandola2009} to detect the contaminated epochs.

The healthy dataset was composed of a combination of control groups from various experiments in order to get a sufficient sample size, however this could cause problems if the data was affected by the fact it was recorded over a long time period (months and years, rather than days). However, given the relative powers were used rather than absolute measures and the artefact rejection was conducted on a `per subject' basis this seems unlikely.



