\chapter{Methodology}

\section{Data cleaning}

The original proposal did not allocate a great deal of time (2-3 weeks) to data cleaning (dealing with any problems the data may have and loading it into a format amenable to further processing) this turned out to be more difficult than expected and as such took significantly longer (4-5 weeks, including a visit to an MEG laboratory to learn more about the data acquisition methods and processing techniques - which proved invaluable).

Most of the data for the healthy control patients was in the same format of a text file consisting of one column per MEG channel and one column for the time vector with each sample being a separate row. However some of the subjects shared the format of the Alzheimer's and MCI patients in which each the data for each channel (and the time vector) were contained within separate folders. Furthermore, some of the subjects had their data in different units as all the data had been multiplied by a factor of $1\times10^{5}$ due to different conventions between the Spanish and English versions of Microsoft Excel which had been used to manipulate the raw data several years ago.

Eleven of the data files were corrupted. Initially it seemed that that it would be possible to still use these subjects - using interpolation to deal with the missing data and rejecting the epochs where the interpolation was used to remove any effect upon the final data. However this ended up being very time-consuming as some of the data had multiple corruptions and some was so corrupted that even attempted to partially load the data failed. Therefore as the eleven subjects represented less than 5\% of the total 233 healthy controls it was decided that it wasn't worth the time it would take to recover the data.

Almost all of the data had been downsampled at acquistion, however for reasons that remain unknown, three uncorrupted subjects and one of the corrupted ones had not been downsampled. As the desired sampling rate was known however, it was easy to downsample these subjects using the \texttt{decimate} function in MATLAB, this automatically filters the data first to satisfy the Nyquist criterion and avoid aliasing.

The \textit{Nyquist criterion} in this context refers to the requirement that the highest frequency contained in the signal is less than half of the sampling rate. If this is not the case then \textit{aliasing} will occur in which the frequency of the sampled signal does not match the frequency of the original continuous signal. This loss of information means that accurate reconstruction of the original signal is impossible. Thus prior to downsampling (reducing the sample rate), the signal is first passed through a low-pass filter with a threshold at the Nyquist frequency ($\frac{1}{2}$ of the new downsampled sampling rate) so that aliasing does not occur. \cite{Smith1999} 
It was decided to use the FieldTrip MATLAB library\cite{Oostenveld2011} designed for EEG/MEG analysis. This would make the artefact rejection easier to perform in a more rigorous and standardised manner. Unfortunately the FieldTrip library expected the data in the format output by the original MEG machine and not the format in which we had recieved the data. After many attempts to load the data it appeared it would not be successful and so we produced our own artefact rejection scripts in MATLAB. This proved to be unnecessary as following a discussion with the Cognitive and Computational Neuroscience group at the Centro de Tecnolog\`{i}a Biom\'{e}dica (CTB), Madrid we discovered that it was possible to 'trick' Fieldtrip into loading the data provided one could provide a data header file with correct sensor information from an actual MEG machine. As the group could provide us with such a file we were then able to successfully load the data into FieldTrip and thus use it for the artefact rejection and signal processing.

The age of the subjects (and MMSE scores for the diseased patients) were stored across various different Excel spreadsheets. As these had been compiled by hand there were some typos in the filenames and differing conventions that made it impossible to fully automate the process of extracting the data into MATLAB. However it was possible to semi-automate it such that only the filenames that didn't match an entry in the spreadsheet required further inspection, reducing the number of files to check from 220 to around 40 and thus significantly reducing the time required.

\section{Artefact Rejection}



